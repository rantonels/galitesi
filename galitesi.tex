\documentclass{article}
%\usepackage[utf8x]{inputenc} 
\usepackage[margin=1.3in]{geometry}
\renewcommand{\baselinestretch}{1.2} 
\usepackage{kpfonts}
\usepackage[parfill]{parskip}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2212}{$-$}

\usepackage{tikz}
\usepackage{transparent}

\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\usepackage{hyperref}
\usepackage{afterpage}
\usepackage{float}
\usepackage{multicol}
    
\usepackage{amsmath,amssymb,amsfonts,amsthm}

\usepackage[url = false]{biblatex}
\bibliography{bibliography}

\usepackage{physics}
\usepackage{mathtools} % arows
\usepackage{color}

\usepackage{xfrac} % fractions like/this

\usepackage{svg}

\usepackage{pgf}
\usepackage{import}


\DeclareMathOperator*{\SumInt}{%
\mathchoice%
  {\ooalign{$\displaystyle\sum$\cr\hidewidth$\displaystyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.14\height}{\scalebox{.7}{$\textstyle\sum$}}\cr\hidewidth$\textstyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
}

\newcommand\inputpgf[2]{{
\let\pgfimageWithoutPath\pgfimage
\renewcommand{\pgfimage}[2][]{\pgfimageWithoutPath[##1]{#1/##2}}
\input{#1/#2}
}}

\newcommand{\ess}{\ensuremath{\mathbb{S}}}
\newcommand{\ar}{\ensuremath{\mathbb{R}}}

\newcommand{\T}{\ensuremath{\vartheta}}
%\renewcommand{\Im}{\operatorname{Im}}

\renewcommand{\deg}{{}^{\circ}}
\newcommand{\hil}{\ensuremath{\mathcal{H}}}
\newcommand{\cmnt}[1]{\textcolor{red}{\emph{#1}}}
\newcommand{\intR}{\int_{-\infty}^\infty}
\newcommand{\sumZ}{\sum_{n=-\infty}^{\infty}}
\newcommand{\locint}{L_{1,\operatorname{loc}}}

\usepackage{graphicx}
\graphicspath{{images/}}

\newcommand{\figurewrapper}[4]{
    \begin{figure}[H]
    \captionsetup{width=0.8\textwidth}
    \centering{
    \def\svgwidth{\linewidth}
    {#1}
    \caption{#3}
    \label{fig:#4}
    }
    \end{figure}
}

\newcommand{\immagine}[4]{
    \figurewrapper{\input{images/#1}}{#2}{#3}{#4}
}

\newcommand{\immaginegraph}[4]{
    \figurewrapper{\includegraphics{#1}}{#2}{#3}{#4}
}



\title{Quantum particle on a circle}
\author{Riccardo Antonelli}

\begin{document}

%\tikz[remember picture,overlay] \node[opacity=0.3,inner sep=0pt] at (current page.center){{\transparent{0.1}\includegraphics[width=0.5\paperwidth,height=0.5\paperwidth]{Stemma_galileiana}}};

\makeatletter
\begin{center} \Large
    \vfill
    {\LARGE \textbf{Universit\`a degli studi di Padova}}\\
    \vspace{20pt}
    {\LARGE Scuola Galileiana di Studi Superiori}\\
    \vspace{20pt}
    {\LARGE \emph{Classe di Scienze Naturali}}\\ 
    \vspace{80pt}
    {\LARGE Tesi di diploma galileiano}\\

    \vspace{80pt}

    {\Huge \bfseries \@title }
    \vfill

\end{center}

\vspace{100pt}

\begin{multicols}{2} \Large
    \begin{flushleft}
    Relatore\\
    Prof. Paolo Ciatti
\end{flushleft}

    \columnbreak

    \begin{flushright}
    Diplomando\\
    \@author
\end{flushright}

\end{multicols}

\vspace{30pt}


\newpage

\maketitle

\begin{abstract}
    We study the probability amplitude for a quantum particle on a circle to return to its original position after a time $t$. This amplitude turns out to be a tempered distribution with an intricate structure of singularities on a dense set and transformation properties reminiscent of a modular form. A connection to the theory of theta functions and the problem of quadratic Gauss sums in number theory is hinted to.
\end{abstract}

\tableofcontents

\pagebreak

\section{Introduction}

Quantum mechanics includes mathematical tools routinely employed in the calculation of measurable quantities that on any remotely formal level appear nonsensical and ill-defined. The root of this widespread misbehaviour of general quantum theory would seem to trace back to the fact that quantum-mechanical observables are to be computed through expressions of the type:

\begin{equation}\label{illdefined}
    \mathcal{O} = \abs{ \sum_{c} e^{i\mathcal{F}[c]} }^2\,.
\end{equation}

The sum is over all possible configurations and can actually be a finite sum, a series, an integral, or even a functional integral, and for essentially all choices of the real functional $\mathcal{F}$ it has very poor convergence. Nevertheless, in physics expressions like \eqref{illdefined} are often produced and manipulated with no concerns for whether they make any sense \emph{temporarily}, and are then handled with an array of regularization procedures until they are carried to a sensible final result for a desired experimental prediction.

Perhaps the most general of such regularizations is Wick rotation, in which a transformation $t \rightarrow it$ in the complex time plane is performed. Generally, computed at purely imaginary time the functionals $\mathcal{F}$ acquire a positive imaginary part and the sum-over-paths becomes convergent. The real-time desired quantity is hopefully recovered through analytic continuation up to the real axis. This is usually understood as an occasion to cure the pathologies of quantum mechanics by \emph{defining} a quantum theory simply as the result of the Wick rotation procedure. This is a practical (and almost always successful) approach but is somewhat unsatisfactory or at least unsettling, as it delegates the description of real quantities to abstract dynamics in imaginary time and also requires strong assumption of analyticity of observables.

We would like here instead to attempt a sketch of alternative meaning that can be assigned to the naively divergent oscillatory sums of quantum mechanics. For this reason we consider the simplest possible quantum system in which such unruly summations occur, a free particle on $\mathbb{S}^1$. This is a sandbox even more basic than the more popular particle in a box\footnote{In fact, the box is a simple orbifold of the circle, as $[0,1] \cong \mathbb{S}^1/\{\theta \sim - \theta\}$. Thus the propagator in a box is easily obtained as a sum of two circle propagators through the method of images.}, and in this context we pose the simplest question:

\vspace{2em}

\begin{quote} \emph{What is the probability that a particle left on a point on the circle will reach another given point after some given time?} \end{quote}

\vspace{2em}

This probability (density) is the squared modulus of a complex amplitude, depending on the final position and elapsed time, called the propagator, which is simply the fundamental solution to the Schr\"odinger equation on the circle. The answer to this apparently absolutely innocuous question is already an incredibly bizzare object. Even after being rigorously defined through regularization, it still does not behave as a function of position and time in any naive sense. Normally, this is kept ``as is'' and read as a formal series, up until the point when one computes more regular observables starting from it. In this work, instead, we insist in providing an accurate description of this amplitude as it comes as an actual mathematical object (and it will turn out to be a tempered distribution of both time and space) and a study of its features.

The propagator will be found to be endowed with an intricate structure of infinite singularities and zeroes that are mapped to eachother by modular transformations, but will nonetheless be perfectly well-defined as a tempered distribution; in fact the sum-over-histories that produces it will be shown to be sensible if understood as a distributional limit. A greater attention will be provided on the time dependence of the amplitude.

Finally, after this anatomical study of the circle propagator, we will offer a possible physical understanding for the features found.

\section{Quantum mechanics on $\ess^1$}

We very briefly review the basic points of the Schr\"odinger representation-picture of quantum mechanical systems with finite degrees of freedom.

\subsection{Schr\"odinger equation and propagators}

Arguably, a minimal description of a quantum mechanical system is in terms of a separable complex Hilbert space $\hil$, the state space, an algebra $\mathcal{A}$ of operators acting on $\hil$, and, if there is interest in studying time-evolution, a selected hermitian operator $H \in \mathcal{A}$, the Hamiltonian \cite{ali_quantization}.  

The state, representing information available about the system, is encoded as a vector\footnote{This is actually only true for \emph{pure} (i.e., maximal) information. Imperfect/incomplete information, that is a mixed state, admits a representation as a state matrix but not as a state space vector. In addition, proportional vectors of $\hil$ map to the same pure state, so that the latter should better be made to correspond with rays of $\hil$. Both of these points are not relevant to our present discussion.} (ket) $\ket{\Psi(t)}$ in state space, where we have explicited the possibility that this knowledge is time-dependent. This possibility is realized, and information on the system at time $t=t_1$ determines the state at a later time $t=t_2$ by integration of the Schr\"odinger equation:

\begin{equation*}
    i \hbar \dv{t}\ket{\Psi(t)} = \hat H \ket{\Psi(t)}
\end{equation*}


If, in addition, we exclude that the Hamiltonian depends explicitly on time, the Schr\"odinger equation can immediately be integrated through an operator exponential:

\begin{equation*}
    \ket{\Psi(t_2)} = e^{-\frac{i}{\hbar} \hat H (t_2-t_1)} \ket{\Psi(t_1)} 
\end{equation*}

$\hat U(t) = e^{-\frac{i}{\hbar} H t}$ is the unitary time-evolution operator, and its determination coincides with the complete solution of the Schr\"odinger equation.

While the presentation above is absolutely general, we specialize to a very simple subcase, that of a mechanical system with finite degrees of freedom. If we have a classical Hamiltonian system whose phase space is a product $M \times TM \ni (q,p)$, where the $n$-dimensional Riemannian smooth manifold $(M,g)$ is called the configuration space, and with some Hamiltonian function $H(p,q)$, then the \textbf{quantization} of this system is constructed by taking as Hilbert space the set of square-normalizable complex wavefunctions on configuration space\footnote{An interesting generalization is to consider instead wavefunctions to be sections of a complex line bundle over $M$, perhaps topologically non-trivial. This can be seen to be equivalent to the introduction of a flat electromagnetic potential with a non-trivial holonomy, and this addition on the $\mathbb{S}^1$ case is a possible future extension for this work.}:

\begin{equation*}
    \hil = L^2(M) = \left\{ \psi : M \rightarrow \mathbb{C} : \int d\operatorname{vol}_M \abs{\psi(q)}^2 < \infty \right\}\,.
\end{equation*}

Then the dynamics can be specified in any given atlas of $M$. In each chart $M \times TM \supset U \rightarrow \mathbb{R}^n \times \mathbb{R}^n$, $(q,p) \mapsto (x,p)$, we represent the classical coordinates $x$, $p$ respectively by the Hermitian operators $\hat x$, $\hat p$ acting as

\begin{equation*}
    \hat x \psi(x)  := x \psi(x)\\
    \hat p \psi(x)  := -i \hbar \operatorname{grad}_M \psi(x)
\end{equation*}

Then finally we can write the Hamiltonian $\hat H$ as simply\footnote{More precisely, $H(p,q)$ is either written as a polynomial or approximated with a family of polynomials, and the same operation is performed on the operators $\hat x$, $\hat p$ to build the quantized Hamiltonian $\hat H$. Since these do not commute, ambiguities might arise, with physical implications. It is not the case for the situations we will consider, however, and we can ignore these issues.} $H(\hat x, \hat p)$. In each chart, the Schr\"odinger equation holds:

\begin{equation*}
    i \pdv{t} \psi(x,t) = \hat H \psi(x,t)
\end{equation*}

Physically speaking, the class of systems above is fairly broad. We can restrict it considerably to Hamiltonians of the form

\begin{equation*}
    H(p,q) = \frac{\left\langle p, p \right\rangle_g}{2m}\,,
\end{equation*}

which describe the dynamics of a free point particle of mass $m$ moving on $(M,g)$. We now choose units in which $\hbar = m = 1$. The Schr\"odinger equation reduces to

\begin{equation} \label{localschroedinger}
    i \pdv{t} \psi(x,t) = - \frac{\nabla^2}{2} \psi(x,t)
\end{equation}

where $\nabla^2$ is the Laplace-Beltrami operator for the metric $g$.

There exists a way of understanding the time-evolution operator $\hat U(t)$ more explicitly in this case. Define the function $G(x_1,x_2;t)$ as the solution to the initial-value problem for equation \eqref{localschroedinger} with a $\delta$-function initial condition:

\begin{align*}
    i \pdv{t} G(x_1;x,t) & = - \frac{\nabla^2_x}{2} G(x_1;x,t)\,, \\
    G(x_1;x,0) & = \frac{1}{\sqrt{g}} \delta(x-x_1)\,.
\end{align*}

Then, since \eqref{localschroedinger} is linear, the solution to the initial-value problem with initial condition $\psi(x,0)$ can be found as a convolution:

\begin{equation}\label{convolutionformula}
    \psi(x,t) = G(x_1;x,t) \ast_{x_1} \psi(x_1,0)\,.
\end{equation}

The function $G$ is known as a fundamental solution or in more physical terms the propagator, since it coincides with the probability amplitude for a particle left at position $x_1$ to be found at position $x$ after time $t$. \eqref{convolutionformula} then describes how knowledge about the distribution of initial positions evolves as a sum over the evolutions of possible initial positions. In any case \eqref{convolutionformula} is the specification of the operator $\hat U(t)$ as a convolution operator.

Taking a basis of eigenstates of position $\ket x$ normalized such that $\int dx \ket{x} \bra{x}$ is the identity operator, note that

\begin{align}
    G(x_1;x,t) & := \bra{x_1} \hat U (t) \ket{x} \nonumber \\
    & = \bra{x_1} e^{-i H t} \ket{x} \nonumber \\
    & = \bra{x_1} e^{-i H t'} e^{-i H (t-t')} \ket{x} \nonumber \\
    & = \int dx' \bra{x_1} e^{-i H t'} \ket{x'} \bra{x'} e^{-iH(t-t')} \ket{x} \nonumber\\
    & = \int dx' G(x_1;x',t')G(x';x,t-t') \label{productform}
\end{align}

Expression \eqref{productform} states the amplitude of propagation from an initial to a final point is equal to the sum over all possible intermediate position of the product between the amplitude of propagation from initial to intermediate and to intermediate to final position. This is a general and fundamental statement about quantum mechanics: amplitudes are sum over possible processes, each of which is a product of amplitudes of subprocesses. In general, one can always write some equality of the form

\newcommand{\iniziale}{\bra{\text{initial}}}
\newcommand{\finale}{\ket{\text{final}}}

\begin{equation}
    \label{amplitudessum}
    \iniziale \hat U(t) \finale = \SumInt_{\text{intermediate}} \iniziale \hat U(t') \ket{\text{intermediate}} \bra{\text{intermediate}} U(t-t') \finale\,;
\end{equation}

where $\SumInt$ can be any general type of "sum", including even finite or infinite sums or integrals under which the intermediate states form a suitably normalized complete basis of $\hil$.


\subsection{Review: free particle on $\mathbb{R}$}\label{sec:free}

Consider the quantization of the simplest possible continuous classical system: the free particle in one dimension. This is the Hamiltonian system phase space $\ar^2$ (with the simple chart $(p,x)$) and the Hamiltonian $H(p,x) = \frac{p^2}{2}$ (having chosen units such that $m=1$). Configuration space is the real line. Consequently, the quantum description has as state a complex wavefunction with domain $\ar$, and evolution is according to the Hamiltonian

\begin{equation*}
    H = - \frac{\partial_x^2}{2}\,.
\end{equation*}

We are interested in calculation of the propagator $G(x_1;x_2,t)$. Because of translation invariance, this quantity can only depend on the difference $x_2-x_1$, and we can simply write $G(x;t)$.

%It is instructive to review the derivation of this Green function through both methods described before.
%
%We begin with the path integral picture.
%
%
%the final expression for the propagator, which we call the Schr\"odinger kernel, is 
%
%\begin{equation}
%    G(t,x) = (2\pi i t)^{-1/2} \exp(\frac{i}{2} \frac{x^2}{t})\,.
%\end{equation}

The most straightforward path to the propagator is through diagonalization of $H$. As a simple derivative operator, it's obvious a basis of eigenfunctions of $H$ is given by the Fourier components $\phi_k(x) = e^{ikx}$. Each of them evolves according to

\begin{equation}
    \label{rfourierevolution}
    \phi_k(x; t) = \exp(\frac{it}{2}\partial_x^2) \, \phi_k(x;0) = e^{-ik^{2}t/2} e^{ikx}\,.
\end{equation}

Our initial wavefunction is the (non-normalizable) delta function: $\Psi(x;0) = \delta(x)$. Recalling the Fourier representation of the distribution $\delta$:

\begin{equation*}
    \Psi(x;0) = \frac{1}{2\pi} \int_{-\infty}^\infty dk \; e^{ikx} 
\end{equation*}

and inserting \eqref{rfourierevolution} we obtain the evolved state at a later time:

\begin{equation}\label{rprop}
    \Psi(x;t)\, =\, \frac{1}{2\pi} \int_{-\infty}^{\infty} dk \; \exp( - i \frac{k^2}{2}t +  ikx )\, =\, (2 \pi i t)^{-1/2} \exp(\frac{ix^2}{2t})\, =: G(x,t)\,;
\end{equation}

where from now on the square root of a purely imaginary number is defined as

\begin{equation*}
    \sqrt{ix} := e^{i(\operatorname{sgn} x) \frac{\pi}{4}} \sqrt{\abs{x}}\,.
\end{equation*}

We note a paradoxical property of the propagator \eqref{rprop}. At any given $t$, $\abs{G(t,x)}^2 \equiv \frac{1}{2\pi t}$ identically for all $x$. The state at time $t>0$ would seem to assign uniform probability density to all positions on the real line, and is therefore non-normalizable. This behaviour becomes more intuitive in light of the Heisenberg uncertainty principle (HUP), which states that for any state the variance of the position and momentum observables satisfy the inequality

\begin{equation*}
    \sigma_x \sigma_p \geq \frac{1}{2}
\end{equation*}

The initial $\delta$-function state is a position eigenstate, and thus has $\sigma_x = 0$. It follows that $\sigma_p = \infty$, and indeed the Fourier transform is the constant function.

If any possible value of the momentum is equally likely, it makes sense that after any small time the particle could have moved to any point on the real line with equal probability. This statement of dubious rigorousness is as far as it is possible to go in the attempt to make physical sense of the propagator as an actual solution. In reality, physically realizable initial states are normalizable and will remain so as they evolve in time; nevertheless it will be possible to model such evolution through convolution with the Schr\"odinger kernel:

\begin{equation*}
    \Phi(x;t) \, = \, \left(\Phi(0) * G(t) \right)(x) \, := \, \intR dx' \Phi(x',0) G(x-x',t)
\end{equation*}

and since the kernel is a tempered distribution\footnote{For positive times, $G(x;t)$ is a smooth bounded function, so it defines a tempered distribution. For $t=0$, it equals a $\delta$-function by definition, again a tempered distribution.}, if $\Phi(x,0)$ is taken to be Schwartz so will be $\Phi(x,t)$. Thus the state will be normalizable at all times.

As an instructive example, consider a more regular initial state, such as the Gaussian $\Phi(x;0) = N \exp(-\frac{x^2}{2\sigma^2} )$, with $N$ such that $\int dx \abs{\Phi(x)}^2 = 1$. The wavefunction at time $t$ is then

\begin{equation*}
    \Phi(x;t) = \left( \Phi(0) * G(t) \right)(x) = \int dy \Phi(y;0) G(x-y,t)
\end{equation*}

\begin{equation*}
    = N \sqrt\frac{\sigma^2}{\sigma^2 + it} e^{-\frac{x^2}{2(\sigma^2 + it)}}\,.
\end{equation*}

The final state is still smooth and normalizable; in fact $\abs{\Phi(x;t)}^2$ is again a gaussian with finite (but growing in time) standard deviation $\sigma^2/(\sigma^4 + t^2)$.

A clearer understanding of the analytic structure of propagator \eqref{rprop} can be reached if a connection with a well-known classical problem is uncovered. Consider again the Schr\"odinger equation for a particle moving on some Riemannian manifold $M$:

\begin{equation*}
    i \pdv{t} \Psi = - \frac{\nabla^2}{2} \Psi\,,
\end{equation*}

where $\nabla^2$ is the Laplace-Beltrami operator on $M$, and perform the substitution $t \mapsto -it$; this yields

\begin{equation*}
    \pdv{t} \Psi = - \frac{\nabla^2}{2} \Psi\,,
\end{equation*}

i.e. the heat / diffusion equation on $M$. The imaginary-time propagator is

\begin{equation}\label{imrprop}
    i G(x;-it) = \frac{1}{\sqrt{2\pi t}} \exp(-\frac{x^2}{t})\,,
\end{equation}

which is the famous and well-understood heat kernel, the fundamental solution to the heat equation on $\ar$. \eqref{imrprop} is only defined for positive $t$, as the heat equation only has smooth solution for infinite time in the future, but it develops singularities in finite time if time-reversed. Mathematically, this corresponds to entering the lower-half plane region in $t$ (understood as complex) where the Gaussian integrals are divergent. The quantum case is exactly on the axis which acts as boundary of convergence.

This can be understood physically as diffusion being the Wick-rotated, or imaginary-time, counterpart of the Schr\"odinger equation. This, in fact, is to be seen as a minimal example of a general phenomenon in quantum theories in which the Wick-rotated theory (a statistical-mechanical system) has better converging observables; and observables in the quantum theory, only formally specified by non-converging oscillatory integrals, can be defined by analytic continuation of the imaginary-time observables.



\subsection{Free particle on the circle}

Let us consider now the case of interest for this work: a particle moving on the cirlce $\ess^1$. A change of units can fix the radius of the circle to any desired value; we take the circumference to be $1$. In a local chart, for example using an angle coordinate $x \in [0,1]$, the Schr\"odinger equation takes the form

\begin{equation}
    \label{schroedeqt}
    -i\partial_t \Psi(x;t) = - \frac{\partial_x^2}{2} \Psi(x;t)
\end{equation}

\begin{equation}    
    \label{boundaryconds}
    \Psi(0;t) = \Psi(1;t), \quad \partial_x \Psi(0;t) = \partial_x \Psi(1;t)
\end{equation}

Boundary conditions \eqref{boundaryconds} should be obvious intuitively; formally they can be derived easily by considering a second overlapping chart and imposing the Schr\"odinger equation applies there too.

Again the goal is to determine the propagator, that is to say the generalized solution to \eqref{schroedeqt} corresponding to a $\delta$-function initial condition.

One way of integrating the equation is by exploiting the known non-compact solution through a method of images. The circle is interpreted as $\ess^1 = \ar / \mathbb{Z}$, that is the quotient of $\ar$ through the identification of points that differ by an integer. Thus, a free particle starting on a point $x_1$ in the circle at time $0$ and ending on another point $x_2$ after a certain time interval $t$ is equivalent to a free particle on $\mathbb{R}$ starting on $x_1 \in \mathbb{R}$ and ending up on \emph{any} representative $x_2 + n$, $n\in\mathbb{Z}$, of the class of $x_2$ under the equivalence relation. The index $n$ of the representative corresponds to the winding number, that is the number of turns around the circle performed by the particle before reaching $x_2$. (See figure \ref{fig:wrapping}).

\immagine{wrapping.pdf_tex}{17cm}{A few trajectories on $\ar$ from $x_1$ to $x_2+n$ and their images under the quotient $\mathbb{S}^1 = \ar/\mathbb{Z}$, which are paths from $x_1$ to $x_2$ with winding number $n$.}{wrapping}

In accordance with \eqref{amplitudessum} the amplitude for the particle on $\ess^1$ to move from $x_1$ to $x_2$ is equal to the sum over all images for the particle on $\ar$ to move from $x_1$ to that image over the same time interval. Therefore, setting again $x_1 = 0$ by translation invariance, the circle propagator is given by the sum over "unwrapped" propagators:

\begin{equation*}
    A(x;t) := A(0,x;0,t) = \sumZ G(x + n; t)\,.
\end{equation*}

Naively ignoring convergence issues, we would then obtain the sum

\begin{equation*}
    A(x;t) = (2\pi i t)^{-1/2} \sumZ \exp(i \frac{(x+ n)^2}{2t})\,.
\end{equation*}

Defining the variables\footnote{The reasoning behind the nature of the redefinitions will become apparent in the next section.} $\tau := - 2\pi t $ and $z := - x$, this becomes

\begin{equation*}
    A(x;t) = \exp(\frac{ix^2}{2t}) \frac{1}{\sqrt{2\pi i t}} \sumZ \exp( \frac{in^2}{2t} + \frac{ixn}{t} ) 
\end{equation*}


\begin{equation} \label{amplitudeztau}
    = e^{-i\pi z^2 \tau}\, \frac{1}{\sqrt{-i\tau}} \sumZ \exp(-\frac{\pi i n^2}{\tau} +  \frac{2\pi i n z}{\tau})   \,;
\end{equation}


in particular, the probability amplitude (density) for the particle to return to its original position ($x=0$) is

\begin{equation}
    A(0;t) = \frac{1}{\sqrt{-i\tau}} \sumZ e^{-{i \pi n^2}/{\tau}} =: \T(\tau)\,.
\end{equation}


The issue with the above is that the sum of oscillatory terms does not converge in any simple sense. It does however converge in the sense of distributions in $t$ to a tempered distribution with highly singular behaviour. Our goal is to provide a complete description of the structure of this distribution. Since the $x$-dependence of the amplitude is not as fascinating as that on $\tau$, we will limit most of our considerations to the $x=0$ case, with no real loss of qualitative structure. We will recover the $x$ dependence after, in the discussion of fractional revivals and the physical interpretation of the amplitude.

Thus, the main focus of our attention for now is directed towards the $\T(\tau)$ "function".


\section{Deconstructing $\T(\tau)$}

The $\T(\tau)$ ``function'' is actually a tempered distribution, a fact whose proof is actually straightforward but which we postpone until section \ref{sec:convergence}, focusing instead on describing its involved structure of zeroes and singularities. Since this singular structure prevents the application of most standard analytic technique, exposing the singularities themselves will require some ingenuity. We will combine a ``modular hopping'' technique as in \cite{boxpdf} with a more rigorous test-function based approach for probing the $\T(\tau)$ distribution.

\subsection{A resummation formula}\label{resummation}

We will first simplify $\T(t)$ considerably. We recall the Poisson resummation formula\cite{bellman}: if $f(x)$ and $\hat f(k)$ are Fourier transforms\footnote{Note we use the ``physicist's'' convention with $\hat f(k) = \int dx\, e^{-ikx} f(x)$, so the argument of $\hat f$ is the wavenumber and $\xi$ is the inverse wavelength.} of eachother, then

\begin{equation}
    \sumZ f(n) = \sum_{\xi=-\infty}^{\infty} \hat f(2\pi\xi)\,,
\end{equation}

provided any of the two series converges at least distributionally. Taking $f(x) = e^{-i\frac{\pi}{\tau} x^2 }$, and thus $\hat f(k) = \sqrt{-i\tau} e^{i \frac{\tau}{4\pi} k^2}$, the equality reads (renaming dummy $\xi$ to $n$):

\begin{equation*}
    \sumZ e^{-i\frac{\pi}{\tau} n^2 } = \sqrt{-i\tau} \sumZ e^{i\tau\pi n^2} \,,
\end{equation*}

so that the $\T(\tau)$ amplitude is rewritten as

\begin{equation}
    \T(\tau) = \sumZ e^{i\tau\pi n^2}
\end{equation}

\subsection{$\Im{\tau}>0$ and the modular group} \label{sec:modular}

It seems natural to try and promote $t$, or equivalently $\tau$, to complex variables. If $\tau$ is in the upper half plane the series is absolutely convergent pointwise, as

\begin{equation*}
    | \exp(\pi i n^2 \tau) | \leq e^{-\pi n^2 \Im\tau}\,.
\end{equation*}

In fact, the series converges to a function holomorphic on the upper half-plane, the Jacobi theta function\cite{bellman}:

\begin{equation}\label{jacobidef}
\vartheta(z;\tau) := \sumZ \exp( \pi i n^2 \tau + 2\pi i n z )\,.
\end{equation}

\figurewrapper{\input{images/plots/theta0.pgf}}{20cm}{Plot of $\T(\tau)$ over the upper-half plane. Contours of $\abs{\T(\tau)}$ (dashed) and $\operatorname{arg}(\T(\tau))$ (solid) are depicted.}{thetaplot}

The issue is that $\T(\tau)$ coincides with the theta function computed on the real axis, outside of its domain of convergence. We argue $\T(t)$ can be constructed as the limit $\lim_{\epsilon\rightarrow 0} \vartheta(0;\tau + i\epsilon)$, where the limit is taken distributionally; that is to say the distribution is specified by the following action on a test function $\varphi(t)$:

\begin{equation*}
    \intR dt \, \T(t) \varphi(t)  := \lim_{\epsilon\rightarrow 0} \intR dt \, \vartheta(0; \tau + i \epsilon) \varphi(t)
\end{equation*}

We will prove there is convergence in this sense in section \ref{sec:convergence}.

$\vartheta(0;\tau)$ has remarkable transformation properties which carry over to $\T(\tau)$. These can be summarized in the following two identities that are of particular interest for the study of $\T(\tau)$:

\begin{align}
    \vartheta(0; \tau + 2) = \vartheta(0; \tau)\,, \label{t2map}\\
    \vartheta(0; -\tfrac{1}{\tau}) = \sqrt{-i\tau} \, \vartheta(0; \tau)\,. \label{smap}
\end{align}

The first is evident from the derived expression. That the function is periodic of period $2$ in $\tau$ means that the propagator at time $t^* = \frac{1}{\pi}$, or recovering units $t^* = 4\pi \frac{mR^2}{\hbar}$, is equal to the propagator at time $0$. In other words, any quantum state on the circle repeats every $t^*$. This fact is less remarkable if one recalls that all energy eigenvalues on the circle, forming a complete basis, evolve in time with the simple phase factor $e^{int/t^*}$, with $n\in \mathbb{N}$, so that all wavefunctions must also share the same periodicity in time. Still, it means a quantum particle placed exactly at one point on a circle (that is, in a position eigenstate) will return to the original position with probability $1$ every $t*$. We will comment more precisely on this fact in section \ref{sec:physical}.

\newcommand{\modg}{PSL(2,\mathbb{Z})}

The second relation is however especially important in combination with the first. The two transformations in terms of $\tau$ belong to the modular group $\modg$, which is composed of all maps of the form

\begin{equation*}
    \tau \rightarrow \frac{a \tau + b}{c\tau + d}\,, \quad a,b,c,d \in \mathbb{Z}\,,\; ad - bc = 1\,.
\end{equation*}

These are all diffeomorphisms from the upper half-plane to itself. They can also be extended to act on the real axis, provided the latter is projectively compactified with a point at infinity as $\hat{\mathbb{R}} = \mathbb{R} \cup \{\infty\}$. The modular group is also realizable as a matrix group: if a modular transformation is represented by the unitary integer matrix

\begin{equation*}
    \mqty( a  b \\ c & d)
\end{equation*}

then composition of modular maps is seen to coincide with matrix multiplication; this translates into a homomorphism from the modular group to $SL(2,\mathbb{Z})$. Since opposite matrices $A$ and $-A$ yield the same modular transformation, this can be turned into an isomorphism by quotienting the sign ambiguity: $SL(2,\mathbb{Z})/\mathbb{Z}_2 = PSL(2,\mathbb{Z})$ is therefore the modular group.

$\modg$ has an obvious but powerful property of mapping rationals to rationals (or better, extended rationals $\hat{\mathbb{Q}} = \mathbb{Q} \cup \{\infty\}$), and moreover this action is transitive, in the sense that given rationals $r$ and $s$ there exists a modular map sending $r\mapsto s$. The modular group is generated by the two transformations:

\begin{align*}
   & T : \tau \rightarrow \tau + 1\,, \quad& T=\mqty(1 & 1 \\ 0 & 1) && \\
   & S : \tau \rightarrow -1/\tau\,, \quad& S=\mqty(0 & -1 \\ 1 & 0) &&
\end{align*}

The Jacobi theta function $\vartheta(z,\tau)$ has well-defined transformation properties under $T$ and $S$, implying the same holds for general modular transformations. However, the transformation under $T$ involves a shift in the variable $z$, which is not especially useful for our purposes. The transformations that do keep $z$ constant are generated by maps \eqref{t2map} and \eqref{smap}:

\begin{align*}
   & T^2 : \tau \rightarrow \tau + 2\,, \quad& T^2=\mqty(1 & 2 \\ 0 & 1) && \\
   & S : \tau \rightarrow -1/\tau\,, \quad& S=\mqty(0 & -1 \\ 1 & 0) &&
\end{align*}

\newcommand{\emodg}{\Lambda}

These do generate a proper subgroup of $PSL(2,\mathbb{Z})$, as we will show now, which we call $\emodg$. Define the notion of \emph{parity} of a reduced fraction $p/q$ as the parity of $pq$. (The parity of $\infty \in \hat{\mathbb{Q}}$ is taken to be the same as $0$, so even). It's clear this parity is invariant under action of $\emodg$; moreover since at least two rationals of opposite parity exist (e.g. $0$ and $1$), transitivity implies $\emodg$ and $PSL(2,\mathbb{Z})$ cannot coincide.

$\emodg$ is an example of congruence subgroup, i.e. a subgroup of the modular group defined by a congruence relation, in our case

\begin{equation*}
    \emodg = \left\{M \in \modg \, : \quad M \equiv T^2 \text{ or } S \mod 2 \right\} 
\end{equation*}

\begin{equation*}
    = \left\{ \mqty(a  b \\ c & d) \in \modg \, : \quad ac \equiv bd \equiv 0 \mod 2 \right\}
\end{equation*}

and is also known in the literature as the theta subgroup\cite{eichler}.

\figurewrapper{\includegraphics[width=0.8\textwidth]{modulargroup}}{20cm}{A representation of the action of $\modg$ on the upper half-plane $\mathbb{H}^2$ as a tessellation of ideal trangles with angles $(30\deg,30\deg,0\deg)$. Any of the triangles acts as a fundamental domain; the ideal $0\deg$ vertices lie on the real axis at rational points. The shading depicts the two orbits of the subgroup $\emodg \subset \modg$; any pair of same-coloured triangles form a fundamental domain for $\emodg$.}{modular}

What is more useful is that there are \emph{only} two orbits, in the sense that any rational $r$ can be mapped through an element of $\emodg$ to either $0$ or $1$ depending on its parity. We consider the following explicit algorithm.

Taken a rational $r = \pm \frac{p}{q}$ in lowest terms, $r\neq 0, 1$,

\begin{itemize}
    \item if $r < -1$ or $r > 1$, apply relevant power of $T^2$ to place it in $[-1,1]$
    \item apply $S$
\end{itemize}

The above procedure has the property of always decreasing the magnitude of either $p$ or $q$. Since these are positive integers, iteration of the procedure has to reach $0$ or $1$ in a finite number of steps. A couple examples:

\newcommand{\sapp}{\,\;\ensuremath{\xrightarrow{\mathmakebox[2em]{S}}}\;\,}
%\newcommand{\tappz}{\ensuremath{\xrightarrow{T^2}}}
\newcommand{\tapp}[1]{\,\;\xrightarrow{\mathmakebox[2em]{(T^2)^{#1}}}\;\,}

\begin{equation*}
    \frac{13}{4} \tapp{-2} - \frac{3}{4} \sapp \frac{4}{3} \tapp{-1} - \frac{2}{3} \sapp \frac{3}{2} \tapp{-1} -\frac{1}{2} \sapp 2 \tapp{-1} 0
\end{equation*}

\begin{equation*}
    \frac{7}{9} \sapp - \frac{9}{7} \tapp{1} \frac{5}{7} \sapp -\frac{7}{5} \tapp{1} \frac{3}{5} \sapp - \frac{5}{3} \tapp{1} \frac{1}{3} \sapp -3 \tapp{2} 1
\end{equation*}

Returning to our physical object of interest, the amplitude, we see that properties \eqref{t2map} and \eqref{smap} relate the behaviour of $\T$ at a rational time\footnote{We are, of course, here referring to the normalized time $\tau$ for simplicity.} with the behaviour at time $0$ or $1$ depending on the parity. Therefore $\T$ has a sort of self-similar structure, and any qualitative features present at the points $0$ and $1$ will be repeated infinitely in two dense sets. For example, we anticipate (though this will be studied more precisely) that the amplitude has a $\tau^{-1/2}$ singularity for $\tau$ close to zero, since for very small time the propagator on a circle is expected to reduce to that of the free particle. If so, then there must also be a similar singularity at all even rationals; therefore $\T$ has a dense set of singularities \cite{intermode_traces}.


\subsection{Even rationals}\label{sec:evens}

\newcommand{\asympt}{(-i\tau)^{-1/2}}

It is easy to show $\T$ ``should'' have a $\sim \tau^{-1/2}$ singularity near $\tau = 0$ \cite{boxpdf}. Indeed, applying  \eqref{smap}:

\begin{equation*}
    \vartheta(\tau) = \sqrt{\frac{1}{-i\tau}} \sumZ e^{-i\pi n^2 \frac{1}{\tau}}\,.
\end{equation*}

Naively, all terms in the sum except $n=0$ are oscillatory and supposedly cancel for small times, and $\vartheta(\tau) \sim \asympt$. This corresponds to physical intuition: the propagator should reduce to the free particle propagator for very short times, in which the particle has not yet "probed" the global structure of the circle. If this is true, however, then similar singularities should be repeated at any even rational point. Thus $\T$ would have infinite singularities, and thus be unbounded, in any given interval. This makes it impossible for it to be ever asymptotic to $\tau^{-1/2}$ in the first place.

The asymptotic behaviour must not of course be interpreted as direct but in a distributional sense. In particular, a more sensible definition is that for a family of Schwartz functions $f_\sigma(\tau) = f(\tau/\sigma)$ the evaluation of $\vartheta$ on $f_\sigma$ is asymptotic to that of $f_\sigma$ on the tempered distribution $A \tau^{-1/2}$ for some constant $A$. In integration against a Schwartz function we expect the secundary singularities at each even rational to matter less and less as we approach the main singularity at $\tau = 0$. We now test explicitly $\vartheta$ against a family of shrinking Gaussians and verify this intuition.

Consider the normalized Gaussian $g_\sigma(\tau) = \frac{1}{\sigma \sqrt{2\pi}} \exp(-\frac{\tau^2}{2\sigma^2})$. The integral of $g_\sigma(\tau)$ against $A \tau^{-1/2}$ is

\begin{equation*}
    \bra{\tau^{-1/2}}\ket{g_\sigma} =  A \intR d\tau \, g_\sigma(\tau) \tau^{-1/2} = \frac{2A}{\sigma\sqrt{2\pi}} 2^{1/4} \sqrt\sigma \int_0^\infty dx e^{-x} x^{-3/4} = \frac{A\Gamma(\frac{1}{4})}{2^{1/4}\sqrt{\pi\sigma}}\,.
\end{equation*}

On the other hand, if $g_\sigma(\tau)$ is integrated against $\vartheta$, we obtain

\begin{equation*}
    \bra{\vartheta}\ket{g_\sigma} = \sumZ \int d\tau \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{\tau^2}{2\sigma^2} + i\pi\tau n^2}  = \sumZ \exp(-\pi^2 \sigma^2 n^4 / 2)\,; 
\end{equation*}

only the asymptotic behaviour for small $\sigma$ is relevant. Defined $N^4 := 2 \pi^{-2} \sigma^{-2}$, the summand is slowly-varying for large $N$. Thus we can approximate as an integral:

\begin{equation*}
    = N \sumZ \frac{1}{N} \exp(-\frac{n^4}{N^4}) \sim N \intR d\xi \, e^{-\xi^4} = 2 N \, \Gamma(\tfrac{5}{4}) = \frac{2^{-3/4}}{\sqrt{\pi\sigma}} \Gamma(\tfrac{1}{4})\,.
\end{equation*}

It is therefore seen%\footnote{The doubt might arise that $\vartheta(\tau)$ also has a component with support in $\{0\}$ that is invisible to the family of gaussian (in the sense that its integral against $g_\sigma$ is independent on $\sigma$). However, this component must be a linear combination of $\delta$ derivatives; the even derivatives do yield positive powers of $\sigma$ and are to be excluded,  }
\ that the action of $\vartheta$ on a Schwartz function shrinking around $\tau=0$ is asymptotic to that of the distribution $\asympt$.

Now that the existence of this singularity has been ascertained, let us verify a reduced copy of it exists at any even rational. Imagine the even rational $a$ gets mapped to the even rational $b$ by one step of the iterative procedure described in section \ref{sec:modular}. Assume it is known that in the vicinity of $b$ there is an inverse square-root singularity:

\begin{equation*}
    \vartheta(b + \delta\tau') \sim K \delta \tau^{-1/2}
\end{equation*}

Now, if $a$ and $b$ are related by a simple translation $(T^2)^n$, then it's obvious this singularity is copied with no change in the constant $K$ by the periodicity property \eqref{t2map}. If instead they are related by the map $S$, and $b = -1/a$, then

\begin{equation*}
    \vartheta(a + \delta\tau) = \sqrt\frac{i}{a} \vartheta\left(-\frac{1}{a+\delta\tau}\right) \sim \sqrt\frac{i}{a} \vartheta(b + \frac{\delta \tau}{a^2}) = \sqrt{ia} K \delta\tau^{-1/2}\,,
\end{equation*}

so that $a$ inherits the singularity of $b$ but suppressed by a factor of $\sqrt{a}$. By simple induction, a singularity appears at any even rational $\pm p/q$, and the constant of proportionality $K_{p/q}$ is equal to the square root of the product of all rational encountered in the procedure towards $\tau = 0$ at each application of $S$; it's clear this is simply $|K_{p/q}| = 1/\sqrt{q}$. So, in the vicinity of $p/q$ the distribution $\vartheta$ behaves as

\begin{equation*}
    \vartheta(\tau) \sim \frac{1}{\sqrt{p-q\tau}}
\end{equation*}

%{è possibile anche fare un calcolo della fase delle singolarità? Credo di sì}

%{Forse no... forse è equivalente al calcolo delle fasi nella \eqref{finalrationalkernel}, che coinvolge number theory e potrebbe non essere fattibile nel caso generale}

\subsection{Odd rationals}\label{sec:oddrationals}

The point $\tau=1$ is equally peculiar. We argue $\vartheta$ vanishes there along with all of its distributional derivatives $\dv[k]{t} \vartheta(\tau)$. Again, the statement as it stands is non-sensical unless it is reformulated in distributional language because of the dense set of singularities. To express it we introduce again a family of shrinking Schwartz functions $f_\sigma(\tau) = f((\tau-1)/\sigma)$, this time centered around $1$. We claim that, for all $k \in \mathbb{N}$,

\begin{equation*}
    \lim_{\sigma \rightarrow 0} \bra{ \dv[k]{t}\vartheta}\ket{ f_\sigma } = 0
\end{equation*}

As before, it suffices to test against shrinking normalized Gaussians $g_\sigma(\tau) = \frac{1}{\sigma\sqrt{2\pi}} \exp( - \frac{\tau^2}{2\sigma^2} )$. We find

\begin{equation}
    \bra{\dv[k]{t}\vartheta}\ket{g_\sigma} = \sumZ (i\pi n^2)^k \frac{1}{\sigma\sqrt{2\pi}} \intR d\tau e^{- \frac{(\tau-1)^2}{2\sigma^2} + i\pi\tau n^2} = \sumZ (i\pi n^2)^k \exp( - \frac{\pi^2 \sigma^2 n^4}{2} + i\pi n^2)
\end{equation}

\begin{equation}\label{alternatingser}
    = (i\pi)^k \sumZ n^{2k} (-1)^n e^{-\pi^2 \sigma^2 n^4/2} =: (i\pi)^k S_k(\sigma)\,.\quad\quad(0^0 = 1)
\end{equation}

The series \eqref{alternatingser} is absolutely convergent for any positive $\sigma$ and natural $k$. Defined again $N^4 := 2/(\pi^2 \sigma^2)$, we intend to show  $\lim_{N \rightarrow \infty} S_k = 0$. First we recognize that $S_k$ it is a residue sum for the poles of the meromorphic function

\begin{equation}
    F_{k,N}(z) = \frac{\pi}{\sin{\pi z}} z^{2k} \exp(-\frac{z^4}{N^4})\,,
\end{equation}

%A heuristic reasoning that points to $\lim_{\sigma \rightarrow 0} S_k(\sigma) = 0$ is as follows. Rewrite the alternating series by arranging terms in pairs:

%\begin{equation}
%    S_k(\sigma) = \sum_m (2m)^k e^{-a^2 (2m)^4}  - (2m-1)^{k-1} e^{-a^2 (2m-1)^4}
%\end{equation}

which indeed has poles at all $n \neq 0$ with residue $(-1)^n n^{2k} e^{-n^4/N^4}$, and another pole at $0$ with residue $1$ for $k=0$ only, so that there is match with $S_k$ for any $k$. By the residue theorem, a partial sum for $S_k$ is $1/(2\pi i)$ the contour integral of $F$ around a loop circling the relevant poles; in the limit the entire sum is equal to the integral along a pair of lines:

\begin{equation}
    2\pi i S_k = -\int_{-\infty}^\infty F_{k,N}(\rho + it) d\rho + \int_\infty^\infty F_{k,N}(\rho-it)d\rho\,;
\end{equation}

And this limit is sensible since $F(z)$ is exponentially decaying for $\abs{z} \rightarrow \infty$ as long as $\arg{z} \neq (m+\frac{1}{2}) \frac{\pi}{2}$. Using $F(-z) = - F(z)$ we reduce the integral to the half-line:

\begin{equation}
    - \pi i S_k = \int_{it}^{it + \infty} \frac{\pi}{\sin{\pi z}} z^{2k} \exp(-\frac{z^4}{N^4})\,dz\,,
\end{equation}

Then, with the substitution $z = N \xi$:

\begin{equation}
    = N^{1+2k} \int_{it/N}^{it/N + \infty} \frac{\pi}{\sin{\pi N \xi}} \xi^{2k} \exp(-\xi^4) d\xi\,.
\end{equation}

Note $t$ was arbitrary, so we can choose to fix a constant $it/N =: K$. In addition, for large enough $N$, $\abs{\sin(\pi N \xi)} = \abs{\sin(\pi N (\Re \xi + iK))} \geq C e^{\pi N K}$ for some constant $C$. Thus the absolute value of the integral is bounded:

\begin{equation}
    \pi \abs{S_k} \leq N^{1+2k} \frac{\pi}{C} e^{-\pi N K} \int_0^{\infty} \abs{\xi}^{2k} \exp(-\Re{\xi^{4}}) d\Re\xi\,,
\end{equation}

and the last integral is a finite constant depending only on $K$. We can finally send $N\rightarrow \infty$ while keeping $K$ constant. Therefore $S_k \rightarrow 0$ for $N\rightarrow \infty$, for all $k$.

Having ascertained the existence of the "flat point" of $\vartheta$ in $\tau = 1$, it is now trivial to verify this structure is copied at all odd rationals through the action of the group $\emodg$.

\subsection{Distributional convergence}\label{sec:convergence}

We are now set to prove that there is convergence in the sense of tempered distributions of the series

\begin{equation}
    \vartheta(\tau) = \sumZ e^{i\pi \tau n^2}\,.
\end{equation}

By this, we mean that defined $\vartheta_N(\tau)$ as the partial sum with $|n| < N$, given any Schwartz function $f(\tau)$ the evaluation of $\vartheta_N$ over $f$ converges to a finite value for $N\rightarrow\infty$. The partial sum evaluations however are

\begin{equation}
    \intR d\tau \, \vartheta_N(\tau) f(\tau) = \sum_{|n|<N} \intR d\tau\, e^{i\pi\tau n^2} f(\tau) = \sum_{|n|<N} \hat f(-\pi n^2)\,,
\end{equation}

having recognized the Fourier transform of $f(\tau)$. We recall that the Fourier transform of a Schwartz function is itself Schwartz. This means $\norm\big{\hat f}_{1,0} = \sup_\tau \abs\big{\tau \hat f(\tau)}$ is finite, and thus $\abs\big{\hat f(\tau)} \leq C/\abs{\tau}$ for some $C>0$, so that the above series is absolutely convergent. Thus, $\T(\tau)$ is a well-defined tempered distribution.

Equivalently, we have just proven that the Fourier transform of the $\T(\tau)$ distribution, which is the ``quadratic $\delta$ comb'':

\begin{equation}
    \hat \T(\omega) = \sumZ \delta(\omega + \pi n^2)
\end{equation}

is a well-defined tempered distribution. Then clearly $\T(\tau)$ is also tempered as the Fourier transform of a tempered distribution.

An alternative proof provides a different presentation of $\T(\tau)$. Consider the following series for real $\tau$:

\begin{equation}
    B(\tau) = \frac{2}{i\pi}\sum_{n=1}^\infty \frac{e^{i\pi\tau n^2} }{n^2}
\end{equation}

\figurewrapper{\inputpgf{images}{batman.pgf}}{40cm}{The real (back) and imaginary (front) parts of the function $B(\tau)$ over a revival period, with some small rational points marked. Note that around even rationals $r$ the function behaves as $\sqrt{\tau - r}$, while at odd $r$ all its derivatives vanish; this matches with our results for its derivative $\T(\tau)$.}{batman}



The series is absoutely convergent and bounded as a function of $\tau$, as $\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$. Therefore, $B(\tau)$ is a well-defined function in $\locint$, and thus defines a tempered distribution. $\T(\tau)$ is the distributional derivative of $B(\tau)$, in the sense

\begin{equation}
    \left\langle\T,\phi\right\rangle =  \left\langle 1 + \dv{\tau}B , \phi \right \rangle = \left\langle 1, \phi \right\rangle - \left\langle B, \dv{\tau} \phi \right\rangle\,,
\end{equation}

and the last expression defines a tempered distribution acting on Schwartz $\phi(\tau)$. $B(\tau)$ is useful because, being continuous and bounded, can be plotted; it gives therefore a graphical confirmation of the ideas presented in the previous sections.



\section{The theta function propagator}

Having now satisfyingly examined the qualitative structure of the propagator for $x=0$ as a function of time, we now return back to our original formulation to include the spatial dependence.

Recovering \eqref{amplitudeztau}, and recalling the definition \eqref{jacobidef} of the Jacobi theta function, we recognize

\begin{equation}
    A(z,\tau) = e^{-i\pi z^2 \tau} \frac{1}{\sqrt{-i\tau}}\, \vartheta\left(\frac{z}{\tau};-\frac{1}{\tau}\right)
\end{equation}

The full Jacobi theta function has more general modular transformation properties than the subcase $\vartheta(\tau) = \vartheta(0;\tau)$ we studied before. In particular, it holds that

\begin{equation} \label{zperiodicity}
    \vartheta(z+1;\tau) = \vartheta(z;\tau)
\end{equation}

\begin{equation} \label{tmap}
    \vartheta(z;\tau + 1) = \vartheta(z+\frac{1}{2}; \tau)
\end{equation}

\begin{equation}\label{smapfull}
    \vartheta\left(\frac{z}{\tau}; -\frac{1}{\tau} \right) = \sqrt{-i\tau} \exp(\frac{\pi}{\tau} i z^2) \T(z;\tau)
\end{equation}

Equations \eqref{zperiodicity} and \eqref{tmap} are immediate; \eqref{smapfull} can again be proven by means of Poisson resummation \cite{bellman}; we omit the derivation for the sake of brevity.

Relation \eqref{smapfull} immediately implies that the fundamental solution $A$ is \emph{directly} a theta function:

\begin{equation}
    A(z,\tau) = \vartheta(z;\tau)
\end{equation}

The Jacobi theta for real values of the period is thus reinterpreted physically as the quantum propagator on the circle. This matches perfectly with the well-known fact that the Jacobi theta for imaginary times $\T(z;i\tau)$ is the heat kernel on the circle \cite{bellman}, nothing else than the compact analogue of the description of Wick-rotation on $\ar$ we provided in section \ref{sec:free}.

\figurewrapper{\input{images/heatkernel.pgf}}{20cm}{The heat kernel on the circle, which is simply $\T(z;it)$ for positive real $t$ and is itself real. It corresponds to the evolution of a point of heat or substance on a circle under the heat or diffusion equation; the limit for $t \rightarrow \infty$ is the thermodynamic equilibrium of the uniform distribution. In the plot two periods of $z$ are depicted.}{heatk}


We now show a remarkable structure of the theta function propagator that should clarify the nature of the singularities in the time dependence. Fixed a rational time $\tau = \sfrac{p}{q}$ in least terms, without loss of generality between $0$ and $1$, rewrite the summation index $n$ as $n = aq + b$, for $a \in \mathbb{Z}$, $b = 0,\ldots,q-1$. The amplitude becomes:

\newcommand{\sumA}{\sum_{a=-\infty}^\infty}
\newcommand{\sumB}{\sum_{b=0}^{q-1}}
\newcommand{\sumBnorm}{\sum_{b=0}^{|q|-1}}
\newcommand{\comb}{\operatorname{III}}

\begin{align}
    \T(z;\tau) = \sumA \sumB \exp(\pi i \frac{p}{q} (aq + b)^2  + 2\pi i (aq+b) z)\\
    = \left( \sumB e^{i\pi \frac{p b^2}{q} + 2\pi i b z} \right) \sumA (-1)^{pqa} e^{2\pi i a q} 
\end{align}

We recognize the sum over $a$ as the Fourier series for a Dirac comb, i.e. a periodic array of $\delta$ functions:

\begin{equation}
    \comb_T(z) := \sumZ \delta(z-nT) = \frac{1}{T} \sum_{k=-\infty}^\infty e^{2\pi i k z}\,,
\end{equation}

so that the sum over $a$ reduces to

\begin{align}
    \frac{1}{q} \comb_{\frac{1}{q}} (z+\Delta)\,,
\end{align}

where $\Delta = 0$ if $\sfrac{p}{q}$ is even, and $\Delta = \frac{1}{2}$ if it's odd. Thus we have the remarkable fact that at any fixed rational time $\tau = \sfrac{p}{q}$, the theta function amplitude is a sum of $q$ $\delta$-functions equally spaced around the circle; the Dirac deltas are then weighted by the phase factor

\begin{equation}
    \sumB \left(e^{i\pi/q}\right)^{pb^2 + 2bzq}
\end{equation}

Since $\comb_{\frac{1}{q}}$ only has support in $z = \frac{c+\Delta}{q}$ for $c = 0,\ldots,q-1$ (understood $\mod 1$), this factor is

\begin{equation}\label{betafactor}
    \beta_{p,q,c} = \sumBnorm \left(e^{i\pi/q}\right)^{pb^2 + 2bc + 2\Delta b}\,,
\end{equation}

Expression \eqref{betafactor} is a generalized quadratic Gauss sum, and its determination is extremely complex. It is however sufficient for our purposes to ascertain the norm $|\beta_{p,q,c}|$. We lift the restriction on $p$ and $q$ to be positive and for $p<q$ for the purpose of the proof. Immediately:

\begin{equation}
    \beta_{p+2kq,q,c} = \beta_{p,q,c}\,,
\end{equation}

so that $p$ can be taken to have $\abs{p} < \abs{q}$. In addition, we make use of the reciprocity formula for generalized quadratic Gauss sums\cite{berndt_gauss}:

\begin{equation}
    \abs{\beta_{p,q,c}} = \sqrt{\abs{\frac{q}{p}}} \; \abs{\beta_{-q,p,c}}\,.
\end{equation}

Having in addition noted that $\abs{\beta_{p,1,c}} = 1$ for all $p,c$, we can induct over all pairs $(p,q)$ and conclude\footnote{We remark the reasoning is strongly reminiscent of that employed in section \ref{sec:evens}, suggesting the calculations are connected. Indeed, a proof of the reciprocity formula can also be provided starting from the modular properties of the theta function\cite{berndt_gauss}.}

\begin{equation}
    |\beta_{p,q,c}| = \sqrt{q}
\end{equation}

%Indeed, defined $r=\exp(i\pi/q)$ and $l=2(c+\Delta)$, we have:
%
%\begin{equation}\label{thenorm}
%    \beta_{p,q,c} \beta^*_{p,q,c} = \sum_{b_1=0}^{q-1} \sum_{b_2=0}^{q-1} \left(e^{i\pi/q}\right)^{p(b_1^2 - b_2^2) + l (b_1 - b_2)} = q     
%\end{equation}
%
%{Nessun progresso per dimostrare questo fatto. Numericamente sembra vero.}


Thus, the fundamental solution at fractional time becomes

\begin{equation}\label{finalrationalkernel}
    \vartheta\left( z ; \,\frac{p}{q} \right) = \frac{1}{\sqrt{q}} \sum_{c=0}^{q-1} \phi_{p,q,c} \, \delta \left(z-\frac{c+\Delta}{q}\right)\,,
\end{equation}

for some phases $\phi_{p,q,c}$, a characterization which will become extremely useful when attempting a physical interpretation of time evolution of an initial state.

We just comment that the prefactor $1/\sqrt{q}$ is expected on the face of unitarity, or conservation of information. The time evolution operator of the Schr\"odinger equation is unitary, and preserves the $L^2$ norm of the wavefunction. Taken an initial smooth wavefunction $\psi(z)$ with compact support in an interval around $0$ smaller than $\frac{1}{2q}$, and normalized such that $\int_{\mathbb{S}^1} dx \abs{\psi(z)}^2 = 1$, time evolution by a fractional time $p/q$ is equivalent with convolution with the $\vartheta(z;\sfrac{p}{q})$ kernel, which results by \eqref{finalrationalkernel} in $q$ non-overlapping copies of the original wavefunction, with some phases and rescaled by $1/\sqrt{q}$. The final squared norm of the wavefunction is thus

\begin{equation}
    \frac{1}{q}\sum_{c=0}^{q-1} 1 = 1\,.
\end{equation}

It is notable that the case $p = 1$ is easily interpreted in terms of the modular transformation property \eqref{smapfull}, which implies

\begin{equation}\label{qcombmodular}
    \vartheta\left(z;\frac{1}{q}\right) = \sqrt{iq} e^{-i\pi q z^2} \T(qz;-q) = \sqrt{iq} e^{-i\pi q z^2} \T(q(z+\Delta);0)\,,
\end{equation}

where again $\Delta = \frac{1}{2} (q \mod 2)$. If we recall that by definition $\T(z;0) = \delta(z)$, then \eqref{qcombmodular} implies for $\T(z;\frac{1}{q})$ the comb structure described in \eqref{finalrationalkernel}. Thus, while the structure of the phases in general might be very complex, the existence of these $\delta$-combs at rational times can be traced back entirely to modular invariance.

%\subsection{The full modular group}
%
%Now that the $z$ dependence has been reintroduced, we can finally consider the action of the entire modular group on the theta amplitude $\vartheta(z;\tau)$. For the action under $T:\tau \mapsto \tau+1$, note
%
%\begin{equation}\label{tmap}
%    \vartheta(z;\tau+1) = \sumZ e^{i\pi n^2 \tau + 2\pi n z} (-1)^{i\pi n^2} = \sumZ e^{i\pi n^2 \tau + 2\pi n z} (-1)^{i\pi n} = \T\left(z+\frac{1}{2}; \tau \right)\,.
%\end{equation}
%
%As an obvious consequence, $\T(z;\tau+2) = \T(z+1;\tau) = \T(z;\tau)$, so that the amplitude is truly periodic in time with period $2$ for all positions. \eqref{tmap} in addition says the amplitude also does repeat after $\Delta\tau = 1$, but shifted by an angle of $\pi$ on the circle.
%
%The transformation property under $S: \tau \mapsto -\frac{1}{\tau}$ must be modified from expression \eqref{smap} to account for non-zero $z$. The derivation can again be performed through Poisson resummation but we omit it for the sake of brevity; the result is\cite{citthetamodular}
%
%\begin{equation}
%    \T(\frac{z}{\tau};\frac{\tau}{})
%\end{equation}


\section{Physical interpretation}\label{sec:physical}

A central concern in modern physics is the nature of the interface between quantum and classical mechanics. The correspondence principle, the idea that quantum systems should reduce to classical in some large quantum numbers limit, has been shown consistently to be too strong of a hypothesis if taken with absolute generality. Instead, the boundary between the class of cases where a concept of classical limit is sensible and those for which it is not has proven to be complex and require special care.

For example, it's tempting to assume the classical limit of a quantum system to be recovered for example by considering ``quasiclassical'' states $\ket \psi$ such that

\begin{itemize}
    \item $\ket \psi$ is reasonably localized in configuration space, as to better approximate a classical trajectory $q(t)$.
    \item In phase space $\ket \psi$ saturates the uncertainty principle, as to best approximate a classical phase space pure state, a single point $(q(t),p(t))$.
    \item Under unitary time evolution, $\ket \psi$ should stay localized around the classical trajectory; it should thus also be quasiclassical in a dynamical sense.
\end{itemize}

The success of this program in a few basic examples can be deceiving. For example, the one-dimensional quantum harmonic oscillator admits coherent states

\begin{equation}
    \ket\alpha = \sum_{n=0}^{\infty} \frac{\alpha^n}{\sqrt{n!}} \ket n
\end{equation}

which satisfy the above properties. In fact, $\abs{\psi(x,t)}^2$ is always a gaussian with fixed width as a function of $x$, whose center oscillates harmonically around $x=0$; in addition the wavepacket always saturates the uncertainty principle in phase space. In fact, the coherent state $\ket \alpha$ is rigidly rotated around the origin in phase space through time evolution, with no change in shape.

This is however too simple of a case. In more general quantum systems, it is impossible to prevent spreading of the wavepacket, and thus to construct semiclassical states according to the standards specified above. The path to the correspondence principle must be modified and these standards relaxed - for example, it is accepted that wavepackets spread, provided it is shown that the characteristic spreading time goes to infinity for large quantum numbers.

What, then, is special about the quantum harmonic oscillator, in that it allows such constant-shape solutions? We argue that it is the following property, which is shared with the particle in a $1D$ box and on a circle: the Hamiltonian has a spectrum composed only of integer multiples of a single energy:

\begin{equation}
    E_n = m_n E\,,\quad m_n \in \mathbb{Z}
\end{equation}

The characteristic of having energies integer multiples of a constant is also true of the hydrogen atom if truncated to principal quantum numbers $n < N$, and of the particle on higher-dimensional boxes and tori if the sides are in rational ratios. Consider the time-evolution of \emph{any} initial state $\ket{\psi(0)}$, decomposed in a complete basis of energy eigenstates:

\begin{equation}
    \ket{\psi(0)} = \sum_n \ket{\phi_n(0)}\,,
\end{equation}

\begin{equation}
    \ket{\psi(t)} = \sum_n e^{-i m_n E t} \ket{\phi_n(0)}\,;
\end{equation}

so that, defined the period $T := 2\pi / E$,

\begin{equation}
    \ket{\psi(t+T)} = \ket{\psi(t)}\,.
\end{equation}

Thus \emph{all} states are reconstructed after a time $T$, a phenomenon dubbed quantum revival. This corresponds to the simple periodicity $\T(z;\tau+2) = \T(z;\tau)$ of the theta amplitude we have already investigated. Systems of this type do not encounter issues with wavepacket spreading because the state is necessarily periodic in time.

Such integer revivals however do not provide a satisfying understanding of the nature of singularities of the propagator on the circle, which do not only appear for integer multiples of the fundamental period $T$, but also for rational multiples of it. We now attempt a physical formulation of the origin of such fractional revivals.

\subsection{Fractional revivals and antirevivals}

Moving back to $\mathbb{S}^1$, equation \eqref{finalrationalkernel} states that at rational time $\tau = p/q$ the propagator is a normalized array of $q$ equally spaced $\delta$-functions, weighted with some phases. The array is shifted by $\frac{1}{2q}$ if $pq$ is odd. Physically, this means that an initial state $\psi(z,0)$ will turn into $q$ smaller ``clones'' at any rational time:

\begin{equation}
    \psi(z,p/q) = \frac{1}{\sqrt q} \sum_{c=0}^{q-1} \phi_{p,q,c} \psi\left(z + \frac{c+\Delta}{q},0\right)\,,
\end{equation}

a phenomenon dubbed fractional revival. This explains the appearance of singularities at even rationals in the $z=0$ case: these were simply the $c=0$ copy of the initial $\delta$-function reforming in the corresponding fractional revival; in the odd case instead the revived copies are shifted and so $\T(0;p/q)$ vanishes. We term the latter case ``antirevivals''.

This phenomenon is harder to understand on a basic physical level. It should be remarked that at the heart of fractional revivals is the hidden modular symmetry as in \eqref{qcombmodular}, so we will now try to provide an intuitive picture of the meaning of this property of the propagator. Our system of a particle on a circle with unit circumference has energy eigenvalues:

\begin{equation}
    E_n = 2\pi^2 n^2\,.
\end{equation}

Note that if one takes a smaller circle, of circumference $1/q$ for some positive integer $q$, the energy levels $E^{S}_n$ will of course be rescaled as

\begin{equation}
    E^{S}_n = 2\pi^2 n^2 q^2 = q^2 E^{S}_n\,.
\end{equation}

\newcommand{\aqeq}{\ensuremath{{\,``\!\!=''}}}

That would suggest that if time is rescaled as $t^{S} = q^{-2} t$, then since the Hamiltonians are ``rescaled'' as $\hat{H}^S \aqeq q^2 \hat H$ one would expect the smaller circle time-evolution to coincide with that of the larger circle over a rescaled time:

\begin{equation}
    \hat{U}(t)\; \aqeq \; \hat{U}^{S}(q^2 t)\,.
\end{equation}

This equality cannot actually be sensible as written, as the two time-evolution operators act on different Hilbert spaces and cannot be compared. But we can certainly attempt a more meaningful equality between propagators. Consider the more explicit map between the circles:

\begin{equation} \label{wrapmap}
    z \mapsto z\,, \quad t \mapsto q^2 t\,.
\end{equation}

This map is actually not a diffeomorphism as it is $q$-to-$1$ on $z$; the larger circle is wrapped $q$ times on the smaller one. Still, if one were to ignore this subtlety, it is reasonable to expect an equality of the type

\begin{equation} \label{boipagator}
    A\left(z;\frac{1}{q}\right) \; \aqeq \; A^S(z;q) = \sqrt{q} A(qz;q)\,.
\end{equation}

In the last equality, we used the fact that the system of a particle on a circle of length $1/q$ is identical to our original system of a unitary circumference, once one reparametrizes as $z \mapsto qz$. Thus, the propagators must coincide\footnote{The factor of $\sqrt{q}$ appears for renormalizability, or in other words because the true invariant is the probablity density $dz \abs{A(z,t)}^2$.}. Equation \eqref{boipagator} is essentially the modular transformation property \eqref{qcombmodular}, the only missing piece being the phase. We argue this addition is a consequence of the winding number of \eqref{wrapmap} and could be accounted for by arguing that the wavefunction on the smaller circle is not necessarily a section of a trivial line $\mathbb{C}$-bundle, in other words in some coordinates the boundary conditions on the smaller circle could be of the form

\begin{equation}
    \psi^S(z + \frac{1}{q}; t) = e^{i\alpha} \psi^S(z;t)\,.
\end{equation}

Accounting for this possible non-triviality, the phase factor $\sqrt{i} e^{i\pi q z^2}$ can be seen to arise naturally.

The above gives a reasonable understanding of the behaviour under the modular $S$ transformation $\tau \mapsto - \frac{1}{\tau}$, which in turn implies the comb structure \eqref{qcombmodular}, but is still fairly abstract and disconnected from a practical approach. A much simpler explanation is possible if one allows for a certain lack of rigour; in particular if we assume that the amplitude, written as a path integral over histories

\begin{equation}
    A(z_1,z_2;T) = \int Dz \,e^{iS[z]} = \int Dz \,e^{i \int_0^T \frac{v^2}{2} dt}\,,
\end{equation}

where the integral is over $z(t)$ such that $z(0) = z_1$, $z(T) = z_2$, is dominated by the contribution of the classical solution, i.e. minima of the action, while large quantum fluctuations around these minima tend to cancel; tentatively one can write down a saddle-point-like expansion as

\begin{equation}
    A \sim e^{iS[z_\text{cl}]} \left( 1 + \int D\delta z\, e^{ i \delta S[\delta z]} \right)\,,\quad z(t) = z_\text{cl}(t) + \delta z(t)\,.
\end{equation}

Thus the amplitude is at least in principle understood as a term expected from the classical equation of motion plus arbitrary quantum corrections. Normally there is one unique classical solution with given boundary conditions. However if it is the case that multiple solutions $z_\text{cl}^{(n)}$ exist, then the corresponding amplitude would generalize to a sum:

\begin{equation}
    A \sim \sum_n e^{i S[z_{cl}^{(n)}]} ( 1 + \text{quantum corrections} \ldots)\,.
\end{equation}

And if in addition the number of solutions is infinite, we could expect the amplitude to diverge, more precisely to concentrate in a ``speck'' centered on the arguments $(z_2-z_1,T)$ that trigger the appearance of the infinite classical solutions, and thus would turn into a $\delta$ function and result in a quantum revival. We call such a situation a caustic \cite{horie_quantum}\cite{boxpdf} in analogy with the phenomenon in optics. In the quantum harmonic oscillator, a caustic appears for $z_2 - z_1 = 0$ and $T \in \frac{\pi}{\omega} \mathbb{Z}$, because all classical solutions $z_A(t) = A \sin(\omega t)$ starting at $z_1 = 0$ for $t=0$ reconverge at $z_2 = 0$ at time $T$. These caustics result in a periodic revival of the wavefunction.

On the circle, however, revivals are much more frequent. If $\tau = \frac{p}{q} \in \mathbb{Q}$, then of all classical solutions $z_v(t) = vt$ (meant $\mod 1$) an infinite number will reconverge at each of the $q$ points $\frac{c}{q}$, namely those with $v \in \mathbb{Z}/p$. The quantum particle on the circle has caustics in a dense set of times.

To conclude our discussion, we present a couple of numerical examinations of the fractional revival phenomenon. In figure \ref{fig:pokies}, the Schr\"odinger equation was solved for an initial gaussian wavepacket through convolution with the $\T$ kernel, and the appearance of fractional copies at rational time is evident. We note that the spread in the initial state offers a natural short-length cutoff\footnote{We are cautious about the usage of the expression ``UV cutoff'', since a short-length cutoff is only such in a relativistic context.}, so that if we were to plot the state at a time $p/q$ with $1/q \ll \sigma_x$ the copies would not be distinguishable. This provides some intuition as to how the propagator can be well-defined and regular when convolved; the small-scale singular behaviour is cut off in integration with a sufficiently regular initial state. In figure \ref{fig:butterfly} we plot the same solution, but as a function of space and time. It is found that reducing the spread in the initial state does make smaller-length structure of the propagator appear, but the graph becomes \emph{less} readable and becomes noisy. This is an informal presentation of the fact that there is no convergence of the propagator as a regular sum. We note that the diagonal lines of $\abs{\psi}$ visible through the plot are simply the ``flat points'' we demonstrated in section \ref{sec:oddrationals}, and are called ``intermode traces'' in \cite{intermode_traces}.

%\immagine{3dplots/1-3.pgf}{20cm}{ }{13}
\figurewrapper{
\input{images/3dplots/0-1.pgf}
\input{images/3dplots/1-5.pgf}
\input{images/3dplots/1-4.pgf}
\input{images/3dplots/1-3.pgf}
\input{images/3dplots/2-5.pgf}
\input{images/3dplots/1-2.pgf}
\input{images/3dplots/3-5.pgf}
\input{images/3dplots/2-3.pgf}
\input{images/3dplots/3-4.pgf}
\input{images/3dplots/4-5.pgf}
\input{images/3dplots/1-1.pgf}
}{20cm}{$\abs{\psi}^2$ for an initial gaussian wavepacket at selected rational times. Note that for odd times the $q$ copies are shifter and $\abs{\psi}^2 \sim 0$ at the original center of the wavepacket.}{pokies}


\figurewrapper{\inputpgf{images/}{butterfly.pgf}}{40cm}{$\abs{\psi(z,\tau)}^2$ for an initial Gaussian state over a full revival period.}{butterfly}



\clearpage
\printbibliography

\end{document}
